{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3cf95b-e11e-4fa0-bbb1-1ba9ea337cd6",
   "metadata": {},
   "source": [
    "## Building simple RAG example with LlamaIndex and Jina AI\n",
    "In this article, you'll learn how to create a RAG (Retrieval-Augmented Generation) system using: \n",
    "LlamaIndex, Jina Embeddings, and the Mixtral-8x7B-Instruct-v0.1 language model, which is available on [HuggingFace](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1).\n",
    "If you need more information on the Mixtral language model, please visit the [Mistral AI website](https://mistral.ai/news/mixtral-of-experts/) or look at the model card on HuggingFace.\n",
    "\n",
    "### What is RAG ?\n",
    "Retrieval Augmented Generation is a technique that combines search with language generation. \n",
    "Here's how it works: an external information retrieval system is used to identify documents likely to provide information relevant to the user's query. \n",
    "These documents, along with the user's request, are then passed on to a text-generating language model, producing a natural response.\n",
    "\n",
    "This method enables a language model to respond to questions and access information from a much larger set of documents than it could see otherwise. \n",
    "The language model only looks at a few relevant sections of the documents when generating responses, which also helps to reduce inexplicable errors.\n",
    "\n",
    "#### Getting started\n",
    "First, install all dependencies:\n",
    "```shell\n",
    "!pip install -U  \\\n",
    "    llama-index  \\\n",
    "    llama-parse \\\n",
    "    python-dotenv \\\n",
    "    llama-index-embeddings-jinaai  \\\n",
    "    llama-index-llms-huggingface  \\\n",
    "    llama-index-vector-stores-qdrant  \\\n",
    "    \"huggingface_hub[inference]\"  \\\n",
    "    datasets\n",
    "```\n",
    "\n",
    "Set up secret key values on `.env` file: \n",
    "```bash\n",
    "JINAAI_API_KEY\n",
    "HF_INFERENCE_API_KEY\n",
    "LLAMA_CLOUD_API_KEY\n",
    "QDRANT_HOST\n",
    "QDRANT_API_KEY\n",
    "```\n",
    "\n",
    "Load all environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e410ee0d-d6c0-4223-a40c-d029cdedf75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0126f5-25b3-495d-8416-a6f0c5e13a97",
   "metadata": {},
   "source": [
    "#### 1 - Connect Jina Embeddings and Mixtral LLM\n",
    "LlamaIndex provides built-in support for the [Jina Embeddings API](https://jina.ai/embeddings/).\n",
    "To use it, you need to initialize the `JinaEmbedding`object with your API Key and model name.\n",
    "\n",
    "For the LLM, you need wrap it in a subclass of `llama_index.llms.CustomLLM` to make it compatible with LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad59807b-80a7-4745-8b75-6cd67f0562e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect embeddings\n",
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "\n",
    "jina_embedding_model = JinaEmbedding(\n",
    "    model=\"jina-embeddings-v2-base-en\",\n",
    "    api_key=os.getenv(\"JINAAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# connect LLM\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "mixtral_llm = HuggingFaceInferenceAPI(\n",
    "    model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    token=os.getenv(\"HF_INFERENCE_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593f19a-6e8a-4391-af2b-5f7462458e4d",
   "metadata": {},
   "source": [
    "#### 2 - Prepare data for RAG\n",
    "This example will use household appliance manuals, which are generally available as [PDF documents](https://www.manua.ls/samsung/wf80f5ebw4w/manual).\n",
    "\n",
    "In the `data` folder, we have three documents, and we will use LlamaParse to extract the textual content from the PDF and use it as a knowledge base in a simple RAG.\n",
    "\n",
    "The [free LlamaIndex Cloud plan](https://cloud.llamaindex.ai/parse) is sufficient for our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f6953e-853b-4e0e-9002-551e4af5a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 80f488cb-4538-4aa6-8453-7f3c65b9e06e\n",
      "Started parsing the file under job_id 39e6f47b-ea60-4bc4-8bcc-44c29ed2a272\n",
      "Started parsing the file under job_id 6801ca57-438b-4146-aada-0f7b1379d2a3\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "llamaparse_api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
    "\n",
    "llama_parse_documents = LlamaParse(api_key=llamaparse_api_key, result_type=\"markdown\").load_data([\n",
    "    \"data/DJ68-00682F_0.0.pdf\", \n",
    "    \"data/F500E_WF80F5E_03445F_EN.pdf\", \n",
    "    \"data/O_ME4000R_ME19R7041FS_AA_EN.pdf\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33daa7-ad9e-424e-ab93-bf821e8c60fb",
   "metadata": {},
   "source": [
    "#### 3 - Store data into Qdrant\n",
    "The code below does the following:\n",
    "- create a vector store with Qdrant client;\n",
    "- get an embedding for each chunk using Jina Embeddings API;\n",
    "- combining `sparse`and `dense` vectors for hybrid search;\n",
    "- stores all data into Qdrant;\n",
    "\n",
    "Hybrid search with Qdrant must be enabled from the beginning - we can simply set `enable_hybrid=True`.\n",
    "\n",
    "> An explanation of using hybrid cloud can be inserted here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c598c56-5660-4cb6-8f8c-39d1be38a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default llamaindex uses OpenAI models\n",
    "# setting embed_model to Jina and llm model to Mixtral\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = jina_embedding_model\n",
    "Settings.llm = mixtral_llm\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    url = os.getenv(\"QDRANT_HOST\"),\n",
    "    api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client, collection_name=\"demo\", enable_hybrid=True, batch_size=20\n",
    ")\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=llama_parse_documents, \n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ebf8a0-c920-45cf-9e9b-82542fd0ce5e",
   "metadata": {},
   "source": [
    "#### 4 - Prepare a prompt\n",
    "Here we will create a custom prompt template.\n",
    "This prompt asks the LLM to use only the context information retrieved from the vector database (Qdrant).\n",
    "\n",
    "**When querying with hybrid mode, we can set `similarity_top_k`and `sparse_top_k` separately:**\n",
    "- `sparse_top_k` represents how many nodes will be retrieved from each dense and sparse query.\n",
    "- `similarity_top_k` controls the final number of returned nodes. In the above setting, we end up with 10 nodes.\n",
    "\n",
    "Then, we assemble the query engine using the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b374f1a-3166-4ef4-bca1-3433ea2b8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"-------------------------------\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-------------------------------\"\n",
    "    \"Given the context information and not prior knowledge,\"\n",
    "    \"answer the query. Please be concise, and complete.\\n\"\n",
    "    \"If the context does not contain an answer to the query,\"\n",
    "    \"respond with \\\"I don't know!\\\".\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)\n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = jina_embedding_model\n",
    "Settings.llm = mixtral_llm\n",
    "\n",
    "# retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    "    sparse_top_k=12,\n",
    "    vector_store_query_mode=\"hybrid\"\n",
    ")\n",
    "\n",
    "# response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    llm=mixtral_llm,\n",
    "    text_qa_template=qa_prompt,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "# query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca1d9d-657f-42e0-b901-d2650d9c4edf",
   "metadata": {},
   "source": [
    "#### Final - Asking questions\n",
    "\n",
    "Now you can ask questions and receive answers based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4173e104-371d-49d1-a4b2-062eeb6ae837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The water temperature is set to 70 ËšC during the Eco Drum Clean cycle. You cannot change the water temperature. However, the temperature for other cycles is not specified in the context.\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"What temperature should I use for my laundry?\")\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee310177-22ad-463c-b7cd-3ad34378d009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
