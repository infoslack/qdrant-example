{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3cf95b-e11e-4fa0-bbb1-1ba9ea337cd6",
   "metadata": {},
   "source": [
    "## Building simple RAG example with LlamaIndex and Jina AI\n",
    "In this article, you'll learn how to create a RAG (Retrieval-Augmented Generation) system using: \n",
    "LlamaIndex, Jina Embeddings, and the Mixtral-8x7B-Instruct-v0.1 language model, which is available on [HuggingFace](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1).\n",
    "If you need more information on the Mixtral language model, please visit the [Mistral AI website](https://mistral.ai/news/mixtral-of-experts/) or look at the model card on HuggingFace.\n",
    "\n",
    "### What is RAG ?\n",
    "Retrieval Augmented Generation is a technique that combines search with language generation. \n",
    "Here's how it works: an external information retrieval system is used to identify documents likely to provide information relevant to the user's query. \n",
    "These documents, along with the user's request, are then passed on to a text-generating language model, producing a natural response.\n",
    "\n",
    "This method enables a language model to respond to questions and access information from a much larger set of documents than it could see otherwise. \n",
    "The language model only looks at a few relevant sections of the documents when generating responses, which also helps to reduce inexplicable errors.\n",
    "\n",
    "#### Getting started\n",
    "First, install all dependencies:\n",
    "```shell\n",
    "!pip install -U  \\\n",
    "    llama-index  \\\n",
    "    python-dotenv \\\n",
    "    llama-index-embeddings-jinaai  \\\n",
    "    llama-index-llms-huggingface  \\\n",
    "    llama-index-vector-stores-qdrant  \\\n",
    "    \"huggingface_hub[inference]\"  \\\n",
    "    datasets\n",
    "```\n",
    "\n",
    "Set up secret key values on `.env` file: \n",
    "```bash\n",
    "JINAAI_API_KEY\n",
    "HF_INFERENCE_API_KEY\n",
    "QDRANT_HOST\n",
    "QDRANT_API_KEY\n",
    "```\n",
    "\n",
    "Load all environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e410ee0d-d6c0-4223-a40c-d029cdedf75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0126f5-25b3-495d-8416-a6f0c5e13a97",
   "metadata": {},
   "source": [
    "#### 1 - Connect Jina Embeddings and Mixtral LLM\n",
    "LlamaIndex provides built-in support for the [Jina Embeddings API](https://jina.ai/embeddings/).\n",
    "To use it, you need to initialize the `JinaEmbedding`object with your API Key and model name.\n",
    "\n",
    "For the LLM, you need wrap it in a subclass of `llama_index.llms.CustomLLM` to make it compatible with LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad59807b-80a7-4745-8b75-6cd67f0562e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect embeddings\n",
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "\n",
    "jina_embedding_model = JinaEmbedding(\n",
    "    model=\"jina-embeddings-v2-base-en\",\n",
    "    api_key=os.getenv(\"JINAAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# connect LLM\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "mixtral_llm = HuggingFaceInferenceAPI(\n",
    "    model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    token=os.getenv(\"HF_INFERENCE_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593f19a-6e8a-4391-af2b-5f7462458e4d",
   "metadata": {},
   "source": [
    "#### 2 - Prepare data for RAG\n",
    "Let's download a document that is ready to use.\n",
    "This small dataset contains the paper on Mistral 7B, already separated into smaller parts.\n",
    "\n",
    "So, let's load it straight from HuggingFace.\n",
    "Then, each chunk is transformed into a LlamaIndex document to be stored in Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c39f53-3aee-45be-9529-83e1fcd77728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for RAG\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"infoslack/mistral-7b-arxiv-paper-chunked\", split=\"train\")\n",
    "data = dataset.to_pandas()\n",
    "df = data[['chunk', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ccec7ed-f17c-4651-87c4-0965700d3009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='77eac010-7cb6-4dec-8902-590ff185d0da', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform each chunk into llamaindex document\n",
    "from llama_index.core import Document\n",
    "\n",
    "docs = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    docs.append(Document(\n",
    "        text=row['chunk'],\n",
    "        source=row['source']\n",
    "    ))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33daa7-ad9e-424e-ab93-bf821e8c60fb",
   "metadata": {},
   "source": [
    "#### 3 - Store data into Qdrant\n",
    "The code below does the following:\n",
    "- create a vector store with Qdrant client;\n",
    "- get an embedding for each chunk using Jina Embeddings API;\n",
    "- stores all data into Qdrant\n",
    "\n",
    "> An explanation of hybrid cloud can be inserted here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0819e846-1108-4047-aeea-1c0e5442e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import qdrant_client\n",
    "\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = jina_embedding_model\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    url = os.getenv(\"QDRANT_HOST\"),\n",
    "    api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"demo\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents=docs, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ebf8a0-c920-45cf-9e9b-82542fd0ce5e",
   "metadata": {},
   "source": [
    "#### 4 - Prepare a prompt\n",
    "Here we will create a custom prompt template.\n",
    "This prompt asks the LLM to use only the context information retrieved from the vector database (Qdrant).\n",
    "\n",
    "Then, we assemble the query engine using the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b374f1a-3166-4ef4-bca1-3433ea2b8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"-------------------------------\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-------------------------------\"\n",
    "    \"Given the context information and not prior knowledge,\"\n",
    "    \"answer the query. Please be concise, and complete.\\n\"\n",
    "    \"If the context does not contain an answer to the query,\"\n",
    "    \"respond with \\\"I don't know!\\\".\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)\n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core import Settings\n",
    "Settings.embed_model = jina_embedding_model\n",
    "Settings.llm = mixtral_llm\n",
    "\n",
    "# retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    llm=mixtral_llm,\n",
    "    text_qa_template=qa_prompt,\n",
    "    response_mode=\"compact\",\n",
    ")\n",
    "\n",
    "# query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca1d9d-657f-42e0-b901-d2650d9c4edf",
   "metadata": {},
   "source": [
    "#### Final - Asking questions\n",
    "\n",
    "Now you can ask questions and receive answers based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4173e104-371d-49d1-a4b2-062eeb6ae837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mistral 7B is a large language model that takes a significant step in balancing the goals of high performance and efficiency. It uses a sliding window attention mechanism to reduce the number of operations and memory usage, making it more affordable and efficient for real-world applications.\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"What is so special about Mistral 7B?\")\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f269bd-5956-4050-8bc1-514ad3a5e783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
